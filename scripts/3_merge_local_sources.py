#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import chardet
import platform
import pandas as pd

networksource_dir = "input/download/net/txt"
mysource_dir = "input/download/my"

OUTPUT_DIR = "output"
LOG_DIR = os.path.join(OUTPUT_DIR, "log")
MERGE_DIR = os.path.join(OUTPUT_DIR, "middle/merge")
LOG_MERGE_DIR = os.path.join(LOG_DIR, "merge")

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(MERGE_DIR, exist_ok=True)
os.makedirs(LOG_MERGE_DIR, exist_ok=True)

NETWORK_M3U = os.path.join(MERGE_DIR, "networksource_total.m3u")
NETWORK_CSV = os.path.join(MERGE_DIR, "networksource_total.csv")
NETWORK_LOG = os.path.join(LOG_MERGE_DIR, "networksource_skipped.log")

MYSOURCE_M3U = os.path.join(MERGE_DIR, "mysource_total.m3u")
MYSOURCE_CSV = os.path.join(MERGE_DIR, "mysource_total.csv")
MYSOURCE_LOG = os.path.join(LOG_MERGE_DIR, "mysource_skipped.log")

SOURCE_MAPPING = {
    "1sddxzb.m3u": "æµå—ç”µä¿¡ç»„æ’­",
    "2sddxdb.m3u": "æµå—ç”µä¿¡å•æ’­",
    "3jnltzb.m3u": "æµå—è”é€šç»„æ’­",
    "4sdqdlt.m3u": "é’å²›è”é€šå•æ’­",
    "5sdyd_ipv6.m3u": "å±±ä¸œç§»åŠ¨å•æ’­",
    "6shyd_ipv6.m3u": "ä¸Šæµ·ç§»åŠ¨å•æ’­",
}

def safe_open(file_path):
    with open(file_path, 'rb') as f:
        raw = f.read()
        enc = chardet.detect(raw)['encoding'] or 'utf-8'
    try:
        text = raw.decode(enc, errors='ignore')
    except:
        text = raw.decode('utf-8', errors='ignore')
    text = text.replace('\x00', '')
    return text.splitlines()

def read_m3u_file(file_path: str):
    channels = []
    try:
        lines = safe_open(file_path)
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if line.startswith("#EXTINF:"):
                info_line = line
                url_line = lines[i + 1].strip() if i + 1 < len(lines) else ""

                content = info_line.replace("#EXTINF:-1", "").replace("#EXTINF:", "").strip()

                attributes = dict(re.findall(r'(\w+)=["]([^"]*)["]', content))

                resolution = attributes.get("resolution", "")

                temp = re.sub(r'\w+="[^"]*"', '', content).strip()
                if "," in temp:
                    display_name = temp.split(",", 1)[1].strip()
                else:
                    display_name = temp.strip()

                channels.append({
                    "é¢‘é“å": display_name,
                    "åœ°å€": url_line,
                    "tvg-id": attributes.get("tvg-id", ""),
                    "tvg-name": attributes.get("tvg-name", ""),
                    "å›½å®¶åˆ†ç»„": attributes.get("tvg-country", ""),
                    "è¯­è¨€åˆ†ç»„": attributes.get("tvg-language", ""),
                    "å›¾æ ‡": attributes.get("tvg-logo", ""),
                    "åŸåˆ†ç»„": attributes.get("group-title", ""),
                    "åˆ†è¾¨ç‡": resolution,
                })
                i += 2
            else:
                i += 1

        print(f"ğŸ“¡ M3Uå·²åŠ è½½ {os.path.basename(file_path)}: {len(channels)} æ¡é¢‘é“")
        return channels

    except Exception as e:
        print(f"âš ï¸ è¯»å–M3Uå¤±è´¥: {file_path}: {e}")
        return []

def read_txt_9_column(file_path: str):
    channels = []
    try:
        lines = safe_open(file_path)
        for ln in lines:
            ln = ln.strip()
            if not ln or "#genre#" in ln:
                continue
            parts = ln.split(",")
            if len(parts) < 9:
                continue

            (
                display_name,
                url,
                logo,
                tvg_name,
                country_group,
                lang_group,
                group,
                tvg_id,
                resolution,
            ) = [x.strip() for x in parts[:9]]

            if not (url.startswith("http://") or url.startswith("https://") or url.startswith("rtsp://")):
                continue

            channels.append({
                "é¢‘é“å": display_name,
                "åœ°å€": url,
                "å›¾æ ‡": logo,
                "tvg-name": tvg_name,
                "å›½å®¶åˆ†ç»„": country_group,
                "è¯­è¨€åˆ†ç»„": lang_group,
                "åŸåˆ†ç»„": group,
                "tvg-id": tvg_id,
                "åˆ†è¾¨ç‡": resolution,
            })

        print(f"ğŸ“¡ TXTå·²åŠ è½½ {os.path.basename(file_path)}: {len(channels)} æ¡é¢‘é“")
        return channels

    except Exception as e:
        print(f"âš ï¸ è¯»å–TXTå¤±è´¥: {file_path}: {e}")
        return []

def merge_all_sources(source_dir, is_m3u=False):
    all_channels = []

    if not os.path.exists(source_dir):
        print(f"âš ï¸ æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
        return []

    print(f"ğŸ“‚ æ‰«æç›®å½•: {source_dir}")

    for file in os.listdir(source_dir):
        file_path = os.path.join(source_dir, file)
        if is_m3u and file.endswith(".m3u"):
            chs = read_m3u_file(file_path)
        elif not is_m3u and file.endswith(".txt"):
            chs = read_txt_9_column(file_path)
        else:
            continue

        for ch in chs:
            ch["æ¥æºæ–‡ä»¶"] = file

        all_channels.extend(chs)

    print(f"\nğŸ“Š åˆå¹¶æ‰€æœ‰é¢‘é“ï¼Œå…± {len(all_channels)} æ¡")
    return all_channels

def write_output_files(channels, output_m3u, output_csv, skipped_log):
    seen_urls = set()
    valid = []
    skipped = []

    for ch in channels:
        url = ch["åœ°å€"].strip()
        if url in seen_urls:
            skipped.append((ch["é¢‘é“å"], url, "é‡å¤URL"))
            continue
        seen_urls.add(url)

        valid.append(ch)

    print(f"âœ… æœ‰æ•ˆé¢‘é“: {len(valid)} æ¡")
    print(f"ğŸš« è·³è¿‡: {len(skipped)} æ¡")

    with open(output_m3u, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        for ch in valid:
            extinf_attrs = []
            if ch.get("tvg-id"):
                extinf_attrs.append(f'tvg-id="{ch["tvg-id"]}"')
            if ch.get("tvg-name"):
                extinf_attrs.append(f'tvg-name="{ch["tvg-name"]}"')
            if ch.get("å›¾æ ‡"):
                extinf_attrs.append(f'tvg-logo="{ch["å›¾æ ‡"]}"')
            if ch.get("åŸåˆ†ç»„"):
                extinf_attrs.append(f'group-title="{ch["åŸåˆ†ç»„"]}"')
            if ch.get("åˆ†è¾¨ç‡"):
                extinf_attrs.append(f'resolution="{ch["åˆ†è¾¨ç‡"]}"')

            extinf_str = " ".join(extinf_attrs)
            f.write(f'#EXTINF:-1 {extinf_str},{ch["é¢‘é“å"]}\n{ch["åœ°å€"]}\n')

    df = pd.DataFrame(valid)
    df = df[["é¢‘é“å", "åœ°å€", "tvg-id", "tvg-name", "å›½å®¶åˆ†ç»„", "è¯­è¨€åˆ†ç»„",
             "å›¾æ ‡", "åŸåˆ†ç»„", "åˆ†è¾¨ç‡", "æ¥æºæ–‡ä»¶"]]
    df.to_csv(output_csv, index=False, encoding='utf-8-sig')

    with open(skipped_log, "w", encoding="utf-8") as f:
        f.write("é¢‘é“å,åœ°å€,è·³è¿‡åŸå› \n")
        for name, url, rsn in skipped:
            f.write(f"{name},{url},{rsn}\n")

    print(f"ğŸ“ è¾“å‡ºï¼š{output_m3u} / {output_csv}")
    print(f"ğŸ“ è·³è¿‡æ—¥å¿—ï¼š{skipped_log}")

if __name__ == "__main__":
    print(f"ğŸ”§ å½“å‰ç³»ç»Ÿ: {platform.system()}ï¼Œè¾“å‡ºç»Ÿä¸€ä¸º UTF-8")

    channels = merge_all_sources(networksource_dir, is_m3u=False)
    if channels:
        write_output_files(channels, NETWORK_M3U, NETWORK_CSV, NETWORK_LOG)
    else:
        print("âš ï¸ æ²¡æœ‰è¯»å–åˆ°ä»»ä½•ç½‘ç»œæºé¢‘é“")

    channels_my = merge_all_sources(mysource_dir, is_m3u=True)
    if channels_my:
        write_output_files(channels_my, MYSOURCE_M3U, MYSOURCE_CSV, MYSOURCE_LOG)
    else:
        print(f"âš ï¸ æ²¡æœ‰è¯»å–åˆ°ä»»ä½• M3U æºé¢‘é“ï¼š{mysource_dir}")
